#!/usr/bin/env python3
"""
training_scheduler.py
è®­ç»ƒè°ƒåº¦å™¨ - æ£€æŸ¥ S3 æ•°æ®å¯ç”¨æ€§åæ‰§è¡Œè®­ç»ƒ

ç‰¹ç‚¹:
1. æ£€æŸ¥ S3 ä¸­æ˜¯å¦æœ‰æŒ‡å®šæ—¥æœŸçš„æ•°æ®
2. åªæœ‰æ•°æ®å°±ç»ªæ‰å¼€å§‹å¤„ç†
3. è‡ªåŠ¨ç»§ç»­ä¸‹ä¸€æ‰¹
"""

import os
import json
import subprocess
import boto3
from datetime import datetime, timedelta
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é…ç½®
WORK_DIR = Path("/home/ubuntu/weather-ai")
STATE_FILE = WORK_DIR / "training_state.json"
S3_BUCKET = "weather-ai-models-de08370c"
SATELLITE_PREFIX = "satellite"
GOVDATA_PREFIX = "govdata"

# è®­ç»ƒé…ç½®
BATCH_SIZE_DAYS = 1  # æ¯æ¬¡å¤„ç† 1 å¤©
EPOCHS_PER_BATCH = 100
TRAINING_START_DATE = "2025-10-01"
TRAINING_END_DATE = "2026-01-27"


def load_state():
    """åŠ è½½è®­ç»ƒçŠ¶æ€"""
    if STATE_FILE.exists():
        with open(STATE_FILE, 'r') as f:
            return json.load(f)
    return {
        "last_processed_date": None,
        "total_batches_completed": 0,
        "total_epochs": 0,
        "waiting_for_data": False,
        "history": []
    }


def save_state(state):
    """ä¿å­˜è®­ç»ƒçŠ¶æ€"""
    with open(STATE_FILE, 'w') as f:
        json.dump(state, f, indent=2, ensure_ascii=False)


def check_data_available(date_str):
    """
    æ£€æŸ¥æŒ‡å®šæ—¥æœŸçš„æ•°æ®æ˜¯å¦åœ¨ S3 ä¸­å°±ç»ª
    é€šè¿‡ .complete æ ‡è®°æ–‡ä»¶åˆ¤æ–­
    """
    date_fmt = date_str.replace("-", "")
    complete_key = f"{SATELLITE_PREFIX}/{date_fmt}/.complete"
    
    try:
        s3 = boto3.client('s3')
        s3.head_object(Bucket=S3_BUCKET, Key=complete_key)
        logger.info(f"âœ… æ•°æ®å°±ç»ª: {date_str}")
        return True
    except Exception:
        logger.info(f"â³ æ•°æ®æœªå°±ç»ª: {date_str}")
        return False


def download_from_s3(date_str):
    """ä» S3 ä¸‹è½½æŒ‡å®šæ—¥æœŸçš„æ•°æ®åˆ°æœ¬åœ°"""
    date_fmt = date_str.replace("-", "")
    
    satellite_dir = WORK_DIR / "satellite_data"
    govdata_dir = WORK_DIR / "govdata"
    
    satellite_dir.mkdir(exist_ok=True)
    govdata_dir.mkdir(exist_ok=True)
    
    logger.info(f"ğŸ“¥ ä¸‹è½½å«æ˜Ÿæ•°æ®: {date_str}")
    
    # ä¸‹è½½å«æ˜Ÿæ•°æ®
    result = subprocess.run([
        "aws", "s3", "sync",
        f"s3://{S3_BUCKET}/{SATELLITE_PREFIX}/{date_fmt}/",
        str(satellite_dir) + "/",
        "--exclude", ".complete"
    ], capture_output=True)
    
    if result.returncode != 0:
        logger.error(f"å«æ˜Ÿæ•°æ®ä¸‹è½½å¤±è´¥: {result.stderr.decode()}")
        return False
    
    # ä¸‹è½½æ”¿åºœæ•°æ®
    logger.info(f"ğŸ“¥ ä¸‹è½½æ”¿åºœæ•°æ®: {date_str}")
    for api in ["rainfall", "temperature", "humidity", "pm25"]:
        s3_key = f"{GOVDATA_PREFIX}/{api}_{date_str}.json"
        local_file = govdata_dir / f"{api}_{date_str}.json"
        
        subprocess.run([
            "aws", "s3", "cp",
            f"s3://{S3_BUCKET}/{s3_key}",
            str(local_file)
        ], capture_output=True)
    
    return True


def preprocess_data():
    """é¢„å¤„ç†æ•°æ®ï¼ˆè£å‰ªæ–°åŠ å¡åŒºåŸŸï¼‰"""
    logger.info("ğŸ”§ é¢„å¤„ç†å«æ˜Ÿæ•°æ®...")
    
    result = subprocess.run(
        ["python", "preprocess_images.py"],
        cwd=str(WORK_DIR),
        capture_output=True
    )
    
    if result.returncode != 0:
        logger.error(f"é¢„å¤„ç†å¤±è´¥: {result.stderr.decode()}")
        return False
    
    return True


def cleanup_raw_data():
    """æ¸…ç†åŸå§‹å«æ˜Ÿæ•°æ®ï¼ˆä¿ç•™é¢„å¤„ç†æ•°æ®ï¼‰"""
    satellite_dir = WORK_DIR / "satellite_data"
    
    for nc_file in satellite_dir.glob("*.nc"):
        nc_file.unlink()
        
    logger.info("ğŸ—‘ï¸ å·²æ¸…ç†åŸå§‹å«æ˜Ÿæ•°æ®")


def train_model(epochs):
    """è¿è¡Œæ¨¡å‹è®­ç»ƒ"""
    logger.info(f"ğŸ§  å¼€å§‹è®­ç»ƒ ({epochs} epochs)...")
    
    result = subprocess.run(
        ["python", "train_rolling_window.py", "--epochs", str(epochs)],
        cwd=str(WORK_DIR),
        capture_output=False
    )
    
    return result.returncode == 0


def sync_model_to_s3():
    """åŒæ­¥æ¨¡å‹åˆ° S3"""
    logger.info("â˜ï¸ åŒæ­¥æ¨¡å‹åˆ° S3...")
    
    result = subprocess.run(
        ["./sync_model_to_s3.sh"],
        cwd=str(WORK_DIR),
        capture_output=True
    )
    
    return result.returncode == 0


def sync_sensor_data_to_s3():
    """åŒæ­¥ä¼ æ„Ÿå™¨æ•°æ®åˆ° S3ï¼Œä¾› API æœåŠ¡å™¨ä½¿ç”¨"""
    logger.info("â˜ï¸ åŒæ­¥ä¼ æ„Ÿå™¨æ•°æ®åˆ° S3...")
    
    sensor_file = WORK_DIR / "real_sensor_data.csv"
    if not sensor_file.exists():
        logger.warning("ä¼ æ„Ÿå™¨æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åŒæ­¥")
        return True
    
    result = subprocess.run([
        "aws", "s3", "cp",
        str(sensor_file),
        f"s3://{S3_BUCKET}/sensor_data/real_sensor_data.csv"
    ], capture_output=True)
    
    if result.returncode == 0:
        logger.info("âœ… ä¼ æ„Ÿå™¨æ•°æ®å·²åŒæ­¥åˆ° S3")
    else:
        logger.error(f"ä¼ æ„Ÿå™¨æ•°æ®åŒæ­¥å¤±è´¥: {result.stderr.decode()}")
    
    return result.returncode == 0


def archive_s3_data(date_str):
    """å°† S3 ä¸­çš„åŸå§‹æ•°æ®ç§»åŠ¨åˆ°å½’æ¡£ç›®å½•"""
    date_fmt = date_str.replace("-", "")
    
    logger.info(f"ğŸ“¦ å½’æ¡£ S3 æ•°æ®: {date_str}")
    
    # ç§»åŠ¨å«æ˜Ÿæ•°æ®
    subprocess.run([
        "aws", "s3", "mv",
        f"s3://{S3_BUCKET}/{SATELLITE_PREFIX}/{date_fmt}/",
        f"s3://{S3_BUCKET}/archived/{SATELLITE_PREFIX}/{date_fmt}/",
        "--recursive"
    ], capture_output=True)
    
    # ç§»åŠ¨æ”¿åºœæ•°æ®
    for api in ["rainfall", "temperature", "humidity", "pm25"]:
        subprocess.run([
            "aws", "s3", "mv",
            f"s3://{S3_BUCKET}/{GOVDATA_PREFIX}/{api}_{date_str}.json",
            f"s3://{S3_BUCKET}/archived/{GOVDATA_PREFIX}/{api}_{date_str}.json"
        ], capture_output=True)


def send_notification(success, date_str, error_msg=None):
    """å‘é€é‚®ä»¶é€šçŸ¥"""
    try:
        if success:
            subprocess.run([
                "python", "-c", f"""
from notification import send_training_success_email
send_training_success_email('', '', {{'date': '{date_str}'}})
"""
            ], cwd=str(WORK_DIR))
        else:
            subprocess.run([
                "python", "-c", f"""
from notification import send_training_failure_email
send_training_failure_email('{error_msg}', 'Batch {date_str}')
"""
            ], cwd=str(WORK_DIR))
    except Exception as e:
        logger.error(f"å‘é€é€šçŸ¥å¤±è´¥: {e}")


def get_next_date(state):
    """è·å–ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„æ—¥æœŸ"""
    if state["last_processed_date"]:
        last = datetime.strptime(state["last_processed_date"], "%Y-%m-%d")
        next_date = last + timedelta(days=1)
    else:
        next_date = datetime.strptime(TRAINING_START_DATE, "%Y-%m-%d")
    
    end_date = datetime.strptime(TRAINING_END_DATE, "%Y-%m-%d")
    
    if next_date > end_date:
        return None  # æ‰€æœ‰æ‰¹æ¬¡å·²å®Œæˆ
    
    return next_date.strftime("%Y-%m-%d")


def run_scheduler(max_batches=None, wait_for_data=True):
    """
    è¿è¡Œè®­ç»ƒè°ƒåº¦å™¨
    
    Args:
        max_batches: æœ€å¤§æ‰¹æ¬¡æ•°ï¼ˆNone = æ— é™åˆ¶ï¼‰
        wait_for_data: æ•°æ®ä¸å¯ç”¨æ—¶æ˜¯å¦ç­‰å¾…
    """
    state = load_state()
    batches_run = 0
    
    logger.info("=" * 60)
    logger.info("ğŸš€ è®­ç»ƒè°ƒåº¦å™¨å¯åŠ¨")
    logger.info(f"å·²å®Œæˆæ‰¹æ¬¡: {state['total_batches_completed']}")
    logger.info(f"ä¸Šæ¬¡å¤„ç†: {state['last_processed_date'] or 'æ— '}")
    logger.info("=" * 60)
    
    while True:
        # æ£€æŸ¥æ‰¹æ¬¡é™åˆ¶
        if max_batches and batches_run >= max_batches:
            logger.info(f"âœ… å·²å®Œæˆ {max_batches} ä¸ªæ‰¹æ¬¡")
            break
        
        # è·å–ä¸‹ä¸€ä¸ªæ—¥æœŸ
        next_date = get_next_date(state)
        
        if next_date is None:
            logger.info("ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å·²å®Œæˆï¼")
            break
        
        logger.info(f"\nğŸ“… æ£€æŸ¥æ—¥æœŸ: {next_date}")
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å¯ç”¨
        if not check_data_available(next_date):
            if wait_for_data:
                logger.info("â³ æ•°æ®æœªå°±ç»ªï¼Œç­‰å¾…ä¸­...")
                state["waiting_for_data"] = True
                save_state(state)
                break  # é€€å‡ºï¼Œç­‰å¾…ä¸‹æ¬¡è°ƒåº¦
            else:
                logger.info("â­ï¸ è·³è¿‡æœªå°±ç»ªçš„æ—¥æœŸ")
                continue
        
        state["waiting_for_data"] = False
        
        # æ‰§è¡Œå¤„ç†æµç¨‹
        try:
            # 1. ä¸‹è½½æ•°æ®
            if not download_from_s3(next_date):
                raise Exception("ä¸‹è½½å¤±è´¥")
            
            # 2. é¢„å¤„ç†
            if not preprocess_data():
                raise Exception("é¢„å¤„ç†å¤±è´¥")
            
            # 3. æ¸…ç†åŸå§‹æ•°æ®
            cleanup_raw_data()
            
            # 4. è®­ç»ƒ
            if not train_model(EPOCHS_PER_BATCH):
                raise Exception("è®­ç»ƒå¤±è´¥")
            
            # 5. åŒæ­¥æ¨¡å‹
            if not sync_model_to_s3():
                raise Exception("æ¨¡å‹åŒæ­¥å¤±è´¥")
            
            # 5.5 åŒæ­¥ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆä¾› API æœåŠ¡å™¨ä½¿ç”¨ï¼‰
            sync_sensor_data_to_s3()
            
            # 6. å½’æ¡£ S3 æ•°æ®
            archive_s3_data(next_date)
            
            # æ›´æ–°çŠ¶æ€
            state["last_processed_date"] = next_date
            state["total_batches_completed"] += 1
            state["total_epochs"] += EPOCHS_PER_BATCH
            state["history"].append({
                "date": next_date,
                "completed_at": datetime.now().isoformat()
            })
            save_state(state)
            
            logger.info(f"âœ… æ‰¹æ¬¡å®Œæˆ: {next_date}")
            batches_run += 1
            
            # å‘é€æˆåŠŸé€šçŸ¥ï¼ˆå¯é€‰ï¼Œæ¯ N æ‰¹å‘ä¸€æ¬¡ï¼‰
            if batches_run % 10 == 0:
                send_notification(True, next_date)
                
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡å¤±è´¥: {e}")
            send_notification(False, next_date, str(e))
            break
    
    # æ‰“å°æœ€ç»ˆçŠ¶æ€
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š è°ƒåº¦å™¨çŠ¶æ€")
    logger.info(f"æœ¬æ¬¡è¿è¡Œ: {batches_run} æ‰¹æ¬¡")
    logger.info(f"ç´¯è®¡å®Œæˆ: {state['total_batches_completed']} æ‰¹æ¬¡")
    logger.info(f"ç´¯è®¡ Epochs: {state['total_epochs']}")
    logger.info("=" * 60)
    
    return state


def show_status():
    """æ˜¾ç¤ºå½“å‰çŠ¶æ€"""
    state = load_state()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š è®­ç»ƒè¿›åº¦")
    print("=" * 60)
    
    # è®¡ç®—è¿›åº¦
    start = datetime.strptime(TRAINING_START_DATE, "%Y-%m-%d")
    end = datetime.strptime(TRAINING_END_DATE, "%Y-%m-%d")
    total_days = (end - start).days + 1
    
    completed = state['total_batches_completed']
    progress = (completed / total_days) * 100 if total_days > 0 else 0
    
    print(f"è®­ç»ƒèŒƒå›´: {TRAINING_START_DATE} ~ {TRAINING_END_DATE}")
    print(f"æ€»å¤©æ•°: {total_days}")
    print(f"å·²å®Œæˆ: {completed}")
    print(f"è¿›åº¦: {progress:.1f}%")
    
    # è¿›åº¦æ¡
    bar_width = 40
    filled = int(bar_width * progress / 100)
    bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)
    print(f"\n[{bar}] {progress:.1f}%")
    
    if state['last_processed_date']:
        print(f"\nä¸Šæ¬¡å¤„ç†: {state['last_processed_date']}")
        next_date = get_next_date(state)
        if next_date:
            available = check_data_available(next_date)
            status = "âœ… å°±ç»ª" if available else "â³ ç­‰å¾…æ•°æ®"
            print(f"ä¸‹ä¸€æ‰¹: {next_date} - {status}")
    
    print("=" * 60)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è®­ç»ƒè°ƒåº¦å™¨")
    parser.add_argument("--status", action="store_true", help="æ˜¾ç¤ºçŠ¶æ€")
    parser.add_argument("--run", type=int, default=None, help="è¿è¡Œ N ä¸ªæ‰¹æ¬¡")
    parser.add_argument("--continuous", action="store_true", help="æŒç»­è¿è¡Œ")
    parser.add_argument("--no-wait", action="store_true", help="æ•°æ®ä¸å¯ç”¨æ—¶ä¸ç­‰å¾…")
    parser.add_argument("--reset", action="store_true", help="é‡ç½®çŠ¶æ€")
    
    args = parser.parse_args()
    
    os.chdir(WORK_DIR)
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    env_file = WORK_DIR / ".env.production"
    if env_file.exists():
        with open(env_file) as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    key, _, value = line.strip().partition('=')
                    os.environ[key] = value.strip('"').strip("'")
    
    if args.status:
        show_status()
    elif args.reset:
        if STATE_FILE.exists():
            STATE_FILE.unlink()
            print("âœ… çŠ¶æ€å·²é‡ç½®")
    elif args.continuous:
        while True:
            state = run_scheduler(wait_for_data=not args.no_wait)
            if state.get("waiting_for_data"):
                import time
                logger.info("ğŸ’¤ ç­‰å¾… 1 å°æ—¶åé‡è¯•...")
                time.sleep(3600)
            else:
                break
    elif args.run:
        run_scheduler(max_batches=args.run, wait_for_data=not args.no_wait)
    else:
        run_scheduler(max_batches=1, wait_for_data=not args.no_wait)
