#!/usr/bin/env python3
"""
training_scheduler.py
è®­ç»ƒè°ƒåº¦å™¨ - æ£€æŸ¥ S3 æ•°æ®å¯ç”¨æ€§åæ‰§è¡Œè®­ç»ƒ

ç‰¹ç‚¹:
1. æ£€æŸ¥ S3 ä¸­æ˜¯å¦æœ‰æŒ‡å®šæ—¥æœŸçš„æ•°æ®
2. åªæœ‰æ•°æ®å°±ç»ªæ‰å¼€å§‹å¤„ç†
3. è‡ªåŠ¨ç»§ç»­ä¸‹ä¸€æ‰¹
"""

import os
import json
import subprocess
import boto3
from datetime import datetime, timedelta
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é…ç½®
WORK_DIR = Path("/home/ubuntu/weather-ai")
STATE_FILE = WORK_DIR / "training_state.json"
S3_BUCKET = "weather-ai-models-de08370c"
SATELLITE_PREFIX = "satellite"
GOVDATA_PREFIX = "govdata"

# è®­ç»ƒé…ç½®
BATCH_SIZE_DAYS = 1  # æ¯æ¬¡å¤„ç† 1 å¤©
EPOCHS_PER_BATCH = 100
TRAINING_START_DATE = "2025-10-01"
TRAINING_END_DATE = "2026-01-27"


def load_state():
    """åŠ è½½è®­ç»ƒçŠ¶æ€"""
    if STATE_FILE.exists():
        with open(STATE_FILE, 'r') as f:
            return json.load(f)
    return {
        "last_processed_date": None,
        "total_batches_completed": 0,
        "total_epochs": 0,
        "waiting_for_data": False,
        "history": []
    }


def save_state(state):
    """ä¿å­˜è®­ç»ƒçŠ¶æ€åˆ°æœ¬åœ°å’Œ S3"""
    # æ·»åŠ æ—¶é—´æˆ³
    state["last_updated"] = datetime.now().isoformat()
    
    # è½¬æ¢ç›‘æ§ä»ªè¡¨ç›˜éœ€è¦çš„æ ¼å¼
    dashboard_state = {
        "currentDate": state.get("last_processed_date"),
        "completedBatches": state.get("total_batches_completed", 0),
        "totalEpochs": state.get("total_epochs", 0),
        "currentPhase": "training" if not state.get("waiting_for_data") else "waiting",
        "phases": [
            {"name": "ä¸‹è½½æ•°æ®", "status": "completed" if state.get("total_batches_completed", 0) > 0 else "pending"},
            {"name": "é¢„å¤„ç†", "status": "completed" if state.get("total_batches_completed", 0) > 0 else "pending"},
            {"name": "è®­ç»ƒ", "status": "running" if not state.get("waiting_for_data") else "pending"},
            {"name": "åŒæ­¥æ¨¡å‹", "status": "completed" if state.get("total_batches_completed", 0) > 0 else "pending"}
        ],
        "diskUsage": None,
        "status": "waiting" if state.get("waiting_for_data") else "running",
        "lastUpdate": state["last_updated"]
    }
    
    # ä¿å­˜æœ¬åœ°
    with open(STATE_FILE, 'w') as f:
        json.dump(state, f, indent=2, ensure_ascii=False)
    
    # ä¸Šä¼ åˆ° S3
    try:
        s3 = boto3.client('s3')
        # ä¸Šä¼ ç›‘æ§ä»ªè¡¨ç›˜æ ¼å¼çš„çŠ¶æ€
        s3.put_object(
            Bucket=S3_BUCKET,
            Key="state/training_state.json",
            Body=json.dumps(dashboard_state, indent=2, ensure_ascii=False),
            ContentType="application/json"
        )
        logger.info("â˜ï¸ çŠ¶æ€å·²åŒæ­¥åˆ° S3")
    except Exception as e:
        logger.warning(f"S3 åŒæ­¥å¤±è´¥: {e}")


def upload_history_to_s3(date_str, metrics):
    """å°†è®­ç»ƒå†å²è®°å½•ä¸Šä¼ åˆ° S3"""
    try:
        s3 = boto3.client('s3')
        
        # è·å–ç°æœ‰å†å²
        try:
            obj = s3.get_object(Bucket=S3_BUCKET, Key="history/training_history.json")
            history = json.loads(obj['Body'].read().decode('utf-8'))
        except Exception:
            history = []
        
        # ç”Ÿæˆæ–° ID
        new_id = max([h.get("id", 0) for h in history], default=0) + 1
        
        # åˆ›å»ºæ–°è®°å½•
        new_record = {
            "id": new_id,
            "timestamp": datetime.now().isoformat(),
            "duration_formatted": "N/A",
            "success": metrics.get("success", True),
            "metrics": {
                "mae": metrics.get("last_val_mae", 0.0),
                "rmse": metrics.get("rmse", 0.0),
                "accuracy": 0.0
            },
            "data_info": {
                "date_range": date_str,
                "sensor_records": 0
            },
            "training_config": {
                "epochs": metrics.get("final_epoch", EPOCHS_PER_BATCH)
            }
        }
        
        history.append(new_record)
        
        # åªä¿ç•™æœ€è¿‘ 50 æ¡
        if len(history) > 50:
            history = history[-50:]
        
        # ä¸Šä¼ 
        s3.put_object(
            Bucket=S3_BUCKET,
            Key="history/training_history.json",
            Body=json.dumps(history, indent=2, ensure_ascii=False),
            ContentType="application/json"
        )
        logger.info(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¸Šä¼ : {date_str}")
        return True
    except Exception as e:
        logger.error(f"ä¸Šä¼ å†å²å¤±è´¥: {e}")
        return False


def check_data_available(date_str):
    """
    æ£€æŸ¥æŒ‡å®šæ—¥æœŸçš„æ•°æ®æ˜¯å¦åœ¨ S3 ä¸­å°±ç»ª
    é€šè¿‡ .complete æ ‡è®°æ–‡ä»¶åˆ¤æ–­
    """
    date_fmt = date_str.replace("-", "")
    complete_key = f"{SATELLITE_PREFIX}/{date_fmt}/.complete"
    
    try:
        s3 = boto3.client('s3')
        s3.head_object(Bucket=S3_BUCKET, Key=complete_key)
        logger.info(f"âœ… æ•°æ®å°±ç»ª: {date_str}")
        return True
    except Exception:
        logger.info(f"â³ æ•°æ®æœªå°±ç»ª: {date_str}")
        return False


def download_from_s3(date_str):
    """ä» S3 ä¸‹è½½æŒ‡å®šæ—¥æœŸçš„æ•°æ®åˆ°æœ¬åœ°"""
    date_fmt = date_str.replace("-", "")
    
    satellite_dir = WORK_DIR / "satellite_data"
    govdata_dir = WORK_DIR / "govdata"
    
    satellite_dir.mkdir(exist_ok=True)
    govdata_dir.mkdir(exist_ok=True)
    
    logger.info(f"ğŸ“¥ ä¸‹è½½å«æ˜Ÿæ•°æ®: {date_str}")
    
    # ä¸‹è½½å«æ˜Ÿæ•°æ®
    result = subprocess.run([
        "aws", "s3", "sync",
        f"s3://{S3_BUCKET}/{SATELLITE_PREFIX}/{date_fmt}/",
        str(satellite_dir) + "/",
        "--exclude", ".complete"
    ], capture_output=True)
    
    if result.returncode != 0:
        logger.error(f"å«æ˜Ÿæ•°æ®ä¸‹è½½å¤±è´¥: {result.stderr.decode()}")
        return False
    
    # ä¸‹è½½æ”¿åºœæ•°æ®
    logger.info(f"ğŸ“¥ ä¸‹è½½æ”¿åºœæ•°æ®: {date_str}")
    for api in ["rainfall", "temperature", "humidity", "pm25"]:
        s3_key = f"{GOVDATA_PREFIX}/{api}_{date_str}.json"
        local_file = govdata_dir / f"{api}_{date_str}.json"
        
        subprocess.run([
            "aws", "s3", "cp",
            f"s3://{S3_BUCKET}/{s3_key}",
            str(local_file)
        ], capture_output=True)
    
    return True


def preprocess_data():
    """é¢„å¤„ç†æ•°æ®ï¼ˆè£å‰ªæ–°åŠ å¡åŒºåŸŸï¼‰"""
    logger.info("ğŸ”§ é¢„å¤„ç†å«æ˜Ÿæ•°æ®...")
    
    result = subprocess.run(
        ["python", "preprocess_images.py"],
        cwd=str(WORK_DIR),
        capture_output=True
    )
    
    if result.returncode != 0:
        logger.error(f"é¢„å¤„ç†å¤±è´¥: {result.stderr.decode()}")
        return False
    
    return True


def cleanup_raw_data():
    """æ¸…ç†åŸå§‹å«æ˜Ÿæ•°æ®ï¼ˆä¿ç•™é¢„å¤„ç†æ•°æ®ï¼‰"""
    satellite_dir = WORK_DIR / "satellite_data"
    
    for nc_file in satellite_dir.glob("*.nc"):
        nc_file.unlink()
        
    logger.info("ğŸ—‘ï¸ å·²æ¸…ç†åŸå§‹å«æ˜Ÿæ•°æ®")


def train_model(date_str, epochs):
    """è¿è¡Œæ¨¡å‹è®­ç»ƒ
    
    Args:
        date_str: è¦è®­ç»ƒçš„æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)
        epochs: è®­ç»ƒè½®æ•°
    """
    logger.info(f"ğŸ§  å¼€å§‹è®­ç»ƒ {date_str} ({epochs} epochs)...")
    
    result = subprocess.run(
        ["python", "train_rolling_window.py", 
         "--start", date_str,
         "--end", date_str,
         "--epochs", str(epochs)],
        cwd=str(WORK_DIR),
        capture_output=False
    )
    
    return result.returncode == 0


def sync_model_to_s3():
    """åŒæ­¥æ¨¡å‹åˆ° S3"""
    logger.info("â˜ï¸ åŒæ­¥æ¨¡å‹åˆ° S3...")
    
    result = subprocess.run(
        ["./sync_model_to_s3.sh"],
        cwd=str(WORK_DIR),
        capture_output=True
    )
    
    return result.returncode == 0


def archive_s3_data(date_str):
    """å°† S3 ä¸­çš„åŸå§‹æ•°æ®ç§»åŠ¨åˆ°å½’æ¡£ç›®å½•"""
    date_fmt = date_str.replace("-", "")
    
    logger.info(f"ğŸ“¦ å½’æ¡£ S3 æ•°æ®: {date_str}")
    
    # ç§»åŠ¨å«æ˜Ÿæ•°æ®
    subprocess.run([
        "aws", "s3", "mv",
        f"s3://{S3_BUCKET}/{SATELLITE_PREFIX}/{date_fmt}/",
        f"s3://{S3_BUCKET}/archived/{SATELLITE_PREFIX}/{date_fmt}/",
        "--recursive"
    ], capture_output=True)
    
    # ç§»åŠ¨æ”¿åºœæ•°æ®
    for api in ["rainfall", "temperature", "humidity", "pm25"]:
        subprocess.run([
            "aws", "s3", "mv",
            f"s3://{S3_BUCKET}/{GOVDATA_PREFIX}/{api}_{date_str}.json",
            f"s3://{S3_BUCKET}/archived/{GOVDATA_PREFIX}/{api}_{date_str}.json"
        ], capture_output=True)


def send_notification(success, date_str, error_msg=None):
    """å‘é€é‚®ä»¶é€šçŸ¥"""
    try:
        # è¯»å–è®­ç»ƒæŒ‡æ ‡
        metrics_file = WORK_DIR / "training_metrics.json"
        metrics = {"date": date_str, "mae": 0.0, "rmse": 0.0, "accuracy": 0.0}
        if metrics_file.exists():
            with open(metrics_file, 'r') as f:
                data = json.load(f)
                metrics["mae"] = data.get("last_val_mae", 0.0)
                metrics["rmse"] = data.get("rmse", 0.0)
        
        # ä¿å­˜ metrics åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_metrics = WORK_DIR / ".temp_metrics.json"
        with open(temp_metrics, 'w') as f:
            json.dump(metrics, f)
        
        # æ„å»º Python è„šæœ¬
        if success:
            python_script = '''
import json
from notification import send_training_success_email
with open(".temp_metrics.json", "r") as f:
    metrics = json.load(f)
send_training_success_email("", "", metrics)
'''
        else:
            python_script = f'''
from notification import send_training_failure_email
send_training_failure_email("{error_msg}", "Batch {date_str}")
'''
        
        # ä½¿ç”¨ bash åŠ è½½ç¯å¢ƒå˜é‡
        shell_cmd = f'''cd {WORK_DIR} && source venv/bin/activate && set -a && source .env.production && set +a && python3 -c '{python_script}' '''
        
        result = subprocess.run(
            ["bash", "-c", shell_cmd],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            logger.info(f"ğŸ“§ é‚®ä»¶é€šçŸ¥å·²å‘é€: {date_str}")
        else:
            logger.warning(f"é‚®ä»¶å‘é€å¯èƒ½å¤±è´¥: {result.stderr}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_metrics.exists():
            temp_metrics.unlink()
            
    except Exception as e:
        logger.error(f"å‘é€é€šçŸ¥å¤±è´¥: {e}")


def get_next_date(state):
    """è·å–ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„æ—¥æœŸ"""
    if state["last_processed_date"]:
        last = datetime.strptime(state["last_processed_date"], "%Y-%m-%d")
        next_date = last + timedelta(days=1)
    else:
        next_date = datetime.strptime(TRAINING_START_DATE, "%Y-%m-%d")
    
    end_date = datetime.strptime(TRAINING_END_DATE, "%Y-%m-%d")
    
    if next_date > end_date:
        return None  # æ‰€æœ‰æ‰¹æ¬¡å·²å®Œæˆ
    
    return next_date.strftime("%Y-%m-%d")


def run_scheduler(max_batches=None, wait_for_data=True):
    """
    è¿è¡Œè®­ç»ƒè°ƒåº¦å™¨
    
    Args:
        max_batches: æœ€å¤§æ‰¹æ¬¡æ•°ï¼ˆNone = æ— é™åˆ¶ï¼‰
        wait_for_data: æ•°æ®ä¸å¯ç”¨æ—¶æ˜¯å¦ç­‰å¾…
    """
    state = load_state()
    batches_run = 0
    
    logger.info("=" * 60)
    logger.info("ğŸš€ è®­ç»ƒè°ƒåº¦å™¨å¯åŠ¨")
    logger.info(f"å·²å®Œæˆæ‰¹æ¬¡: {state['total_batches_completed']}")
    logger.info(f"ä¸Šæ¬¡å¤„ç†: {state['last_processed_date'] or 'æ— '}")
    logger.info("=" * 60)
    
    while True:
        # æ£€æŸ¥æ‰¹æ¬¡é™åˆ¶
        if max_batches and batches_run >= max_batches:
            logger.info(f"âœ… å·²å®Œæˆ {max_batches} ä¸ªæ‰¹æ¬¡")
            break
        
        # è·å–ä¸‹ä¸€ä¸ªæ—¥æœŸ
        next_date = get_next_date(state)
        
        if next_date is None:
            logger.info("ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å·²å®Œæˆï¼")
            break
        
        logger.info(f"\nğŸ“… æ£€æŸ¥æ—¥æœŸ: {next_date}")
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å¯ç”¨
        if not check_data_available(next_date):
            if wait_for_data:
                logger.info("â³ æ•°æ®æœªå°±ç»ªï¼Œç­‰å¾…ä¸­...")
                state["waiting_for_data"] = True
                save_state(state)
                break  # é€€å‡ºï¼Œç­‰å¾…ä¸‹æ¬¡è°ƒåº¦
            else:
                logger.info("â­ï¸ è·³è¿‡æœªå°±ç»ªçš„æ—¥æœŸ")
                continue
        
        state["waiting_for_data"] = False
        
        # æ‰§è¡Œå¤„ç†æµç¨‹
        try:
            # 1. ä¸‹è½½æ•°æ® (å¢é‡ä¸‹è½½ï¼Œå·²å­˜åœ¨çš„æ–‡ä»¶ä¼šè·³è¿‡)
            if not download_from_s3(next_date):
                raise Exception("ä¸‹è½½å¤±è´¥")
            
            # 2. é¢„å¤„ç†
            if not preprocess_data():
                raise Exception("é¢„å¤„ç†å¤±è´¥")
            
            # 3. è®­ç»ƒ (ä¼ å…¥æ—¥æœŸå‚æ•°)
            if not train_model(next_date, EPOCHS_PER_BATCH):
                raise Exception("è®­ç»ƒå¤±è´¥")
            
            # 4. æ¸…ç†åŸå§‹æ•°æ® (è®­ç»ƒæˆåŠŸåå†æ¸…ç†ï¼Œé¿å…å¤±è´¥æ—¶é‡å¤ä¸‹è½½)
            cleanup_raw_data()
            
            # 5. åŒæ­¥æ¨¡å‹
            if not sync_model_to_s3():
                raise Exception("æ¨¡å‹åŒæ­¥å¤±è´¥")
            
            # 6. å½’æ¡£ S3 æ•°æ®
            archive_s3_data(next_date)
            
            # æ›´æ–°çŠ¶æ€
            state["last_processed_date"] = next_date
            state["total_batches_completed"] += 1
            state["total_epochs"] += EPOCHS_PER_BATCH
            state["history"].append({
                "date": next_date,
                "completed_at": datetime.now().isoformat()
            })
            save_state(state)
            
            # è¯»å–è®­ç»ƒæŒ‡æ ‡å¹¶ä¸Šä¼ åˆ° S3 å†å²
            metrics_file = WORK_DIR / "training_metrics.json"
            if metrics_file.exists():
                with open(metrics_file, 'r') as f:
                    metrics = json.load(f)
                upload_history_to_s3(next_date, metrics)
            
            logger.info(f"âœ… æ‰¹æ¬¡å®Œæˆ: {next_date}")
            batches_run += 1
            
            # å‘é€æˆåŠŸé€šçŸ¥
            send_notification(True, next_date)
                
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡å¤±è´¥: {e}")
            send_notification(False, next_date, str(e))
            break
    
    # æ‰“å°æœ€ç»ˆçŠ¶æ€
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š è°ƒåº¦å™¨çŠ¶æ€")
    logger.info(f"æœ¬æ¬¡è¿è¡Œ: {batches_run} æ‰¹æ¬¡")
    logger.info(f"ç´¯è®¡å®Œæˆ: {state['total_batches_completed']} æ‰¹æ¬¡")
    logger.info(f"ç´¯è®¡ Epochs: {state['total_epochs']}")
    logger.info("=" * 60)
    
    return state


def show_status():
    """æ˜¾ç¤ºå½“å‰çŠ¶æ€"""
    state = load_state()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š è®­ç»ƒè¿›åº¦")
    print("=" * 60)
    
    # è®¡ç®—è¿›åº¦
    start = datetime.strptime(TRAINING_START_DATE, "%Y-%m-%d")
    end = datetime.strptime(TRAINING_END_DATE, "%Y-%m-%d")
    total_days = (end - start).days + 1
    
    completed = state['total_batches_completed']
    progress = (completed / total_days) * 100 if total_days > 0 else 0
    
    print(f"è®­ç»ƒèŒƒå›´: {TRAINING_START_DATE} ~ {TRAINING_END_DATE}")
    print(f"æ€»å¤©æ•°: {total_days}")
    print(f"å·²å®Œæˆ: {completed}")
    print(f"è¿›åº¦: {progress:.1f}%")
    
    # è¿›åº¦æ¡
    bar_width = 40
    filled = int(bar_width * progress / 100)
    bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)
    print(f"\n[{bar}] {progress:.1f}%")
    
    if state['last_processed_date']:
        print(f"\nä¸Šæ¬¡å¤„ç†: {state['last_processed_date']}")
        next_date = get_next_date(state)
        if next_date:
            available = check_data_available(next_date)
            status = "âœ… å°±ç»ª" if available else "â³ ç­‰å¾…æ•°æ®"
            print(f"ä¸‹ä¸€æ‰¹: {next_date} - {status}")
    
    print("=" * 60)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è®­ç»ƒè°ƒåº¦å™¨")
    parser.add_argument("--status", action="store_true", help="æ˜¾ç¤ºçŠ¶æ€")
    parser.add_argument("--run", type=int, default=None, help="è¿è¡Œ N ä¸ªæ‰¹æ¬¡")
    parser.add_argument("--continuous", action="store_true", help="æŒç»­è¿è¡Œ")
    parser.add_argument("--no-wait", action="store_true", help="æ•°æ®ä¸å¯ç”¨æ—¶ä¸ç­‰å¾…")
    parser.add_argument("--reset", action="store_true", help="é‡ç½®çŠ¶æ€")
    
    args = parser.parse_args()
    
    os.chdir(WORK_DIR)
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    env_file = WORK_DIR / ".env.production"
    if env_file.exists():
        with open(env_file) as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    key, _, value = line.strip().partition('=')
                    os.environ[key] = value.strip('"').strip("'")
    
    if args.status:
        show_status()
    elif args.reset:
        if STATE_FILE.exists():
            STATE_FILE.unlink()
            print("âœ… çŠ¶æ€å·²é‡ç½®")
    elif args.continuous:
        while True:
            state = run_scheduler(wait_for_data=not args.no_wait)
            if state.get("waiting_for_data"):
                import time
                logger.info("ğŸ’¤ ç­‰å¾… 1 å°æ—¶åé‡è¯•...")
                time.sleep(3600)
            else:
                break
    elif args.run:
        run_scheduler(max_batches=args.run, wait_for_data=not args.no_wait)
    else:
        run_scheduler(max_batches=1, wait_for_data=not args.no_wait)
